{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from visdom import Visdom\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import Reader\n",
    "\n",
    "from dotmap import DotMap\n",
    "\n",
    "config = DotMap({\n",
    "    \"training\": True,\n",
    "    \"batch\": 32,\n",
    "    \"workers\": 0,\n",
    "    \"epoch\": 400\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1 convolution\n",
    "def conv_one(in_channel, out_channel):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm3d(in_channel),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv3d(in_channel, out_channel, 1, bias=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel=None):\n",
    "        super(Residual, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        if out_channel is None:\n",
    "            out_channel = in_channel\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            conv_one(in_channel, out_channel // 2),\n",
    "            nn.BatchNorm3d(out_channel // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(out_channel // 2, out_channel // 2, 3, padding=1, bias=False),\n",
    "            conv_one(out_channel // 2, out_channel)\n",
    "        )\n",
    "        \n",
    "        self.skip_layer = None\n",
    "        if in_channel != out_channel:\n",
    "            self.skip_layer = nn.Conv3d(in_channel, out_channel, 1)\n",
    "        \n",
    "        self.out_relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        \n",
    "        out = self.conv_block(x)\n",
    "        \n",
    "        if self.skip_layer is not None:\n",
    "            skip = self.skip_layer(x)\n",
    "        \n",
    "        out += skip\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourglass\n",
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, size, channels):\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.skip_layers = nn.ModuleList([Residual(self.channels) for _ in range(self.size)])\n",
    "        self.low_layers = nn.ModuleList([Residual(self.channels) for _ in range(self.size)])\n",
    "        self.mid = Residual(self.channels)\n",
    "        self.up_layers = nn.ModuleList([Residual(self.channels) for _ in range(self.size)])\n",
    "        self.max_pool = nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2))\n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 2, 2), mode='trilinear', align_corners=False)\n",
    "        # nearest not working with tuple(1, 2, 2). so bilinear\n",
    "\n",
    "    def forward(self, x):\n",
    "        inner = x\n",
    "\n",
    "        skip_outputs = list()\n",
    "        for skip, low in zip(self.skip_layers, self.low_layers):\n",
    "            s = skip(inner)\n",
    "            skip_outputs.append(s)\n",
    "            inner = self.max_pool(inner)\n",
    "            inner = low(inner)\n",
    "        \n",
    "        out = self.mid(inner)\n",
    "\n",
    "        for skip, up in zip(reversed(skip_outputs), reversed(self.up_layers)):\n",
    "            out = skip + self.upsample(up(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, features, internal_size=4):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = features\n",
    "        self.internal_size = internal_size\n",
    "        \n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, 7, stride=(1, 2, 2), padding=3, bias=False),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU(),\n",
    "            Residual(8, 16),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            Residual(16, 32),\n",
    "            Residual(32, self.features)\n",
    "        )\n",
    "        \n",
    "        self.hourglass = Hourglass(self.internal_size, self.features)\n",
    "        \n",
    "        self.out_conv = nn.Sequential(\n",
    "            Residual(self.features, self.features),\n",
    "            conv_one(self.features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        init = self.init_conv(x)\n",
    "           \n",
    "        hg = self.hourglass(init)\n",
    "        \n",
    "        out = self.out_conv(hg)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    Reader.Liver3LayerSH(\n",
    "        augment = True\n",
    "    ),\n",
    "    config.batch,\n",
    "    shuffle=(config.training),\n",
    "    pin_memory=True,\n",
    "    num_workers=config.workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjdtn5683/.conda/envs/pytorch/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator max_pool3d because torch.onnx.symbolic.max_pool3d does not exist\n",
      "  .format(op_name, op_name))\n",
      "/home/tjdtn5683/.conda/envs/pytorch/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator upsample_trilinear3d because torch.onnx.symbolic.upsample_trilinear3d does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = Model(64)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)#lr=0.00025)\n",
    "criterion = nn.BCELoss(torch.ones(config.batch, dtype=torch.float64))\n",
    "# cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "training = True\n",
    "\n",
    "visual = Visdom()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#Batch, Channel, Depth, Height, Width\n",
    "dummy_input = torch.autograd.Variable(\n",
    "    torch.rand(config.batch, 1, 3, 256, 256)\n",
    ")\n",
    "dummy_input = dummy_input.to(device)\n",
    "writer.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 613/613 [10:54<00:00,  1.25 iter/s, loss=0.571]\n",
      "Epoch 2: 100%|██████████| 613/613 [10:44<00:00,  1.27 iter/s, loss=0.525]\n",
      "Epoch 3: 100%|██████████| 613/613 [10:39<00:00,  1.27 iter/s, loss=0.0191]\n",
      "Epoch 4: 100%|██████████| 613/613 [10:37<00:00,  1.27 iter/s, loss=0.908]\n",
      "Epoch 5: 100%|██████████| 613/613 [10:33<00:00,  1.28 iter/s, loss=0.0115]\n",
      "Epoch 6: 100%|██████████| 613/613 [10:32<00:00,  1.27 iter/s, loss=0.287]\n",
      "Epoch 7: 100%|██████████| 613/613 [10:31<00:00,  1.27 iter/s, loss=0.448]\n",
      "Epoch 8: 100%|██████████| 613/613 [10:26<00:00,  1.28 iter/s, loss=0.768]\n",
      "Epoch 9: 100%|██████████| 613/613 [10:26<00:00,  1.28 iter/s, loss=0.0155]\n",
      "Epoch 10: 100%|██████████| 613/613 [10:27<00:00,  1.28 iter/s, loss=0.68]\n",
      "Epoch 11: 100%|██████████| 613/613 [10:27<00:00,  1.29 iter/s, loss=0.314]\n",
      "Epoch 12: 100%|██████████| 613/613 [10:25<00:00,  1.29 iter/s, loss=0.00727]\n",
      "Epoch 13: 100%|██████████| 613/613 [10:25<00:00,  1.28 iter/s, loss=0.378]\n",
      "Epoch 14: 100%|██████████| 613/613 [10:25<00:00,  1.29 iter/s, loss=0.0131]\n",
      "Epoch 15: 100%|██████████| 613/613 [10:22<00:00,  1.30 iter/s, loss=0.507]\n",
      "Epoch 16: 100%|██████████| 613/613 [10:20<00:00,  1.29 iter/s, loss=0.00619]\n",
      "Epoch 17: 100%|██████████| 613/613 [10:21<00:00,  1.30 iter/s, loss=0.00835]\n",
      "Epoch 18: 100%|██████████| 613/613 [10:23<00:00,  1.29 iter/s, loss=0.336]\n",
      "Epoch 19: 100%|██████████| 613/613 [10:21<00:00,  1.30 iter/s, loss=0.393]\n",
      "Epoch 20: 100%|██████████| 613/613 [10:21<00:00,  1.29 iter/s, loss=0.237]\n",
      "Epoch 21: 100%|██████████| 613/613 [10:20<00:00,  1.30 iter/s, loss=0.006]\n",
      "Epoch 22: 100%|██████████| 613/613 [10:20<00:00,  1.29 iter/s, loss=0.262]\n",
      "Epoch 23: 100%|██████████| 613/613 [10:19<00:00,  1.30 iter/s, loss=0.889]\n",
      "Epoch 24: 100%|██████████| 613/613 [10:21<00:00,  1.30 iter/s, loss=0.206]\n",
      "Epoch 25: 100%|██████████| 613/613 [10:20<00:00,  1.30 iter/s, loss=1.01]\n",
      "Epoch 26: 100%|██████████| 613/613 [10:20<00:00,  1.30 iter/s, loss=0.296]\n",
      "Epoch 27: 100%|██████████| 613/613 [10:20<00:00,  1.30 iter/s, loss=0.493]\n",
      "Epoch 28: 100%|██████████| 613/613 [10:18<00:00,  1.31 iter/s, loss=0.245]\n",
      "Epoch 29: 100%|██████████| 613/613 [10:19<00:00,  1.31 iter/s, loss=0.00411]\n",
      "Epoch 30: 100%|██████████| 613/613 [10:19<00:00,  1.30 iter/s, loss=0.0048]\n",
      "Epoch 31: 100%|██████████| 613/613 [10:19<00:00,  1.31 iter/s, loss=0.984]\n",
      "Epoch 32: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.168]\n",
      "Epoch 33: 100%|██████████| 613/613 [10:18<00:00,  1.30 iter/s, loss=0.918]\n",
      "Epoch 34: 100%|██████████| 613/613 [10:19<00:00,  1.31 iter/s, loss=0.00331]\n",
      "Epoch 35: 100%|██████████| 613/613 [10:15<00:00,  1.30 iter/s, loss=0.815]\n",
      "Epoch 36: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.288]\n",
      "Epoch 37: 100%|██████████| 613/613 [10:19<00:00,  1.30 iter/s, loss=0.256]\n",
      "Epoch 38: 100%|██████████| 613/613 [10:18<00:00,  1.32 iter/s, loss=0.876]\n",
      "Epoch 39: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=1.51]\n",
      "Epoch 40: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00164]\n",
      "Epoch 41: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.23]\n",
      "Epoch 42: 100%|██████████| 613/613 [10:18<00:00,  1.31 iter/s, loss=0.00418]\n",
      "Epoch 43: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.171]\n",
      "Epoch 44: 100%|██████████| 613/613 [10:17<00:00,  1.30 iter/s, loss=0.0029]\n",
      "Epoch 45: 100%|██████████| 613/613 [10:17<00:00,  1.31 iter/s, loss=0.155]\n",
      "Epoch 46: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.00266]\n",
      "Epoch 47: 100%|██████████| 613/613 [10:17<00:00,  1.31 iter/s, loss=0.00526]\n",
      "Epoch 48: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.768]\n",
      "Epoch 49: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.0018]\n",
      "Epoch 50: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.00434]\n",
      "Epoch 51: 100%|██████████| 613/613 [10:17<00:00,  1.30 iter/s, loss=0.312]\n",
      "Epoch 52: 100%|██████████| 613/613 [10:18<00:00,  1.31 iter/s, loss=0.227]\n",
      "Epoch 53: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.0023]\n",
      "Epoch 54: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.00353]\n",
      "Epoch 55: 100%|██████████| 613/613 [10:17<00:00,  1.30 iter/s, loss=0.189]\n",
      "Epoch 56: 100%|██████████| 613/613 [10:18<00:00,  1.30 iter/s, loss=0.00218]\n",
      "Epoch 57: 100%|██████████| 613/613 [10:17<00:00,  1.30 iter/s, loss=0.25]\n",
      "Epoch 58: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.766]\n",
      "Epoch 59: 100%|██████████| 613/613 [10:17<00:00,  1.31 iter/s, loss=0.00434]\n",
      "Epoch 60: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.0959]\n",
      "Epoch 61: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.242]\n",
      "Epoch 62: 100%|██████████| 613/613 [10:17<00:00,  1.31 iter/s, loss=0.00526]\n",
      "Epoch 63: 100%|██████████| 613/613 [10:17<00:00,  1.30 iter/s, loss=0.00246]\n",
      "Epoch 64: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.227]\n",
      "Epoch 65: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00191]\n",
      "Epoch 66: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00111]\n",
      "Epoch 67: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.00164]\n",
      "Epoch 68: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.432]\n",
      "Epoch 69: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.471]\n",
      "Epoch 70: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00157]\n",
      "Epoch 71: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.00257]\n",
      "Epoch 72: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.125]\n",
      "Epoch 73: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.00281]\n",
      "Epoch 74: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.135]\n",
      "Epoch 75: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00445]\n",
      "Epoch 76: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.00138]\n",
      "Epoch 77: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00454]\n",
      "Epoch 78: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.289]\n",
      "Epoch 79: 100%|██████████| 613/613 [10:14<00:00,  1.30 iter/s, loss=0.618]\n",
      "Epoch 80: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00197]\n",
      "Epoch 81: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.322]\n",
      "Epoch 82: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.0037]\n",
      "Epoch 83: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.142]\n",
      "Epoch 84: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.00148]\n",
      "Epoch 85: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.338]\n",
      "Epoch 86: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.294]\n",
      "Epoch 87: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.529]\n",
      "Epoch 88: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.0012]\n",
      "Epoch 89: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00141]\n",
      "Epoch 90: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00194]\n",
      "Epoch 91: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.519]\n",
      "Epoch 92: 100%|██████████| 613/613 [10:16<00:00,  1.31 iter/s, loss=0.78]\n",
      "Epoch 93: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.00162]\n",
      "Epoch 94: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00095]\n",
      "Epoch 95: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.442]\n",
      "Epoch 96: 100%|██████████| 613/613 [10:16<00:00,  1.30 iter/s, loss=0.38]\n",
      "Epoch 97: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.228]\n",
      "Epoch 98: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00176]\n",
      "Epoch 99: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00239]\n",
      "Epoch 100: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00112]\n",
      "Epoch 101: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.437]\n",
      "Epoch 102: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.13]\n",
      "Epoch 103: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.206]\n",
      "Epoch 104: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.309]\n",
      "Epoch 105: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00279]\n",
      "Epoch 106: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00205]\n",
      "Epoch 107: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.28]\n",
      "Epoch 108: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.392]\n",
      "Epoch 109: 100%|██████████| 613/613 [10:13<00:00,  1.30 iter/s, loss=0.00099]\n",
      "Epoch 110: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.721]\n",
      "Epoch 111: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.496]\n",
      "Epoch 112: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00183]\n",
      "Epoch 113: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.000679]\n",
      "Epoch 114: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.000994]\n",
      "Epoch 115: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00112]\n",
      "Epoch 116: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.281]\n",
      "Epoch 117: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00129]\n",
      "Epoch 118: 100%|██████████| 613/613 [10:13<00:00,  1.30 iter/s, loss=0.277]\n",
      "Epoch 119: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.138]\n",
      "Epoch 120: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=1.02]\n",
      "Epoch 121: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.229]\n",
      "Epoch 122: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.385]\n",
      "Epoch 123: 100%|██████████| 613/613 [10:14<00:00,  1.30 iter/s, loss=0.00231]\n",
      "Epoch 124: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00064]\n",
      "Epoch 125: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.32]\n",
      "Epoch 126: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00125]\n",
      "Epoch 127: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00276]\n",
      "Epoch 128: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.274]\n",
      "Epoch 129: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.00335]\n",
      "Epoch 130: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.229]\n",
      "Epoch 131: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00107]\n",
      "Epoch 132: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000782]\n",
      "Epoch 133: 100%|██████████| 613/613 [10:15<00:00,  1.32 iter/s, loss=0.476]\n",
      "Epoch 134: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00208]\n",
      "Epoch 135: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.572]\n",
      "Epoch 136: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.591]\n",
      "Epoch 137: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.0014]\n",
      "Epoch 138: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00117]\n",
      "Epoch 139: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.002]\n",
      "Epoch 140: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.365]\n",
      "Epoch 141: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000868]\n",
      "Epoch 142: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.0011]\n",
      "Epoch 143: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.0969]\n",
      "Epoch 144: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.188]\n",
      "Epoch 145: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.535]\n",
      "Epoch 146: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.254]\n",
      "Epoch 147: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.478]\n",
      "Epoch 148: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000949]\n",
      "Epoch 149: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00057]\n",
      "Epoch 150: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.00246]\n",
      "Epoch 151: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00313]\n",
      "Epoch 152: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.346]\n",
      "Epoch 153: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00261]\n",
      "Epoch 154: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.416]\n",
      "Epoch 155: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.487]\n",
      "Epoch 156: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.000545]\n",
      "Epoch 157: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00267]\n",
      "Epoch 158: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.572]\n",
      "Epoch 159: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00466]\n",
      "Epoch 160: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.0976]\n",
      "Epoch 161: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.482]\n",
      "Epoch 162: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000669]\n",
      "Epoch 163: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.0019]\n",
      "Epoch 164: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.192]\n",
      "Epoch 165: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.903]\n",
      "Epoch 166: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000785]\n",
      "Epoch 167: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.0021]\n",
      "Epoch 168: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.699]\n",
      "Epoch 169: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000485]\n",
      "Epoch 170: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.000527]\n",
      "Epoch 171: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00103]\n",
      "Epoch 172: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.113]\n",
      "Epoch 173: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.404]\n",
      "Epoch 174: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00163]\n",
      "Epoch 175: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.44]\n",
      "Epoch 176: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.389]\n",
      "Epoch 177: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000807]\n",
      "Epoch 178: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.0962]\n",
      "Epoch 179: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00146]\n",
      "Epoch 180: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.208]\n",
      "Epoch 181: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.634]\n",
      "Epoch 182: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00049]\n",
      "Epoch 183: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.529]\n",
      "Epoch 184: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00425]\n",
      "Epoch 185: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.000825]\n",
      "Epoch 186: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00195]\n",
      "Epoch 187: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00156]\n",
      "Epoch 188: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00181]\n",
      "Epoch 189: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.341]\n",
      "Epoch 190: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000543]\n",
      "Epoch 191: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000382]\n",
      "Epoch 192: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000606]\n",
      "Epoch 193: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00125]\n",
      "Epoch 194: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00212]\n",
      "Epoch 195: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00188]\n",
      "Epoch 196: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.0027]\n",
      "Epoch 197: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00216]\n",
      "Epoch 198: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.179]\n",
      "Epoch 199: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000914]\n",
      "Epoch 200: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.163]\n",
      "Epoch 201: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.00285]\n",
      "Epoch 202: 100%|██████████| 613/613 [10:10<00:00,  1.31 iter/s, loss=0.00452]\n",
      "Epoch 203: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.276]\n",
      "Epoch 204: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.116]\n",
      "Epoch 205: 100%|██████████| 613/613 [10:15<00:00,  1.30 iter/s, loss=0.00116]\n",
      "Epoch 206: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000436]\n",
      "Epoch 207: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.484]\n",
      "Epoch 208: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.641]\n",
      "Epoch 209: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000957]\n",
      "Epoch 210: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.319]\n",
      "Epoch 211: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.000933]\n",
      "Epoch 212: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00106]\n",
      "Epoch 213: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.00228]\n",
      "Epoch 214: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000575]\n",
      "Epoch 215: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.123]\n",
      "Epoch 216: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00178]\n",
      "Epoch 217: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00134]\n",
      "Epoch 218: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.348]\n",
      "Epoch 219: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000636]\n",
      "Epoch 220: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000643]\n",
      "Epoch 221: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.274]\n",
      "Epoch 222: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00168]\n",
      "Epoch 223: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.0011]\n",
      "Epoch 224: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.423]\n",
      "Epoch 225: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.216]\n",
      "Epoch 226: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00235]\n",
      "Epoch 227: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.0977]\n",
      "Epoch 228: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00226]\n",
      "Epoch 229: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.277]\n",
      "Epoch 230: 100%|██████████| 613/613 [10:09<00:00,  1.33 iter/s, loss=0.000789]\n",
      "Epoch 231: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.757]\n",
      "Epoch 232: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.0019]\n",
      "Epoch 233: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000781]\n",
      "Epoch 234: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.233]\n",
      "Epoch 235: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.514]\n",
      "Epoch 236: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00147]\n",
      "Epoch 237: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.317]\n",
      "Epoch 238: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000406]\n",
      "Epoch 239: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000526]\n",
      "Epoch 240: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00109]\n",
      "Epoch 241: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.157]\n",
      "Epoch 242: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.808]\n",
      "Epoch 243: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.333]\n",
      "Epoch 244: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000521]\n",
      "Epoch 245: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00116]\n",
      "Epoch 246: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.000774]\n",
      "Epoch 247: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000968]\n",
      "Epoch 248: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00197]\n",
      "Epoch 249: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.235]\n",
      "Epoch 250: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.00214]\n",
      "Epoch 251: 100%|██████████| 613/613 [10:10<00:00,  1.31 iter/s, loss=0.000958]\n",
      "Epoch 252: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.757]\n",
      "Epoch 253: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00108]\n",
      "Epoch 254: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.508]\n",
      "Epoch 255: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.308]\n",
      "Epoch 256: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.000445]\n",
      "Epoch 257: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.00134]\n",
      "Epoch 258: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.000686]\n",
      "Epoch 259: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.000975]\n",
      "Epoch 260: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.14]\n",
      "Epoch 261: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.472]\n",
      "Epoch 262: 100%|██████████| 613/613 [10:09<00:00,  1.32 iter/s, loss=0.0022]\n",
      "Epoch 263: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00118]\n",
      "Epoch 264: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.159]\n",
      "Epoch 265: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00145]\n",
      "Epoch 266: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.205]\n",
      "Epoch 267: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00183]\n",
      "Epoch 268: 100%|██████████| 613/613 [10:09<00:00,  1.32 iter/s, loss=0.261]\n",
      "Epoch 269: 100%|██████████| 613/613 [10:10<00:00,  1.31 iter/s, loss=0.264]\n",
      "Epoch 270: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00142]\n",
      "Epoch 271: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.00138]\n",
      "Epoch 272: 100%|██████████| 613/613 [10:13<00:00,  1.30 iter/s, loss=0.678]\n",
      "Epoch 273: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.232]\n",
      "Epoch 274: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.711]\n",
      "Epoch 275: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.9]\n",
      "Epoch 276: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000427]\n",
      "Epoch 277: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.0019]\n",
      "Epoch 278: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.176]\n",
      "Epoch 279: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.00387]\n",
      "Epoch 280: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.0872]\n",
      "Epoch 281: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.000449]\n",
      "Epoch 282: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.269]\n",
      "Epoch 283: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.546]\n",
      "Epoch 284: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.585]\n",
      "Epoch 285: 100%|██████████| 613/613 [10:09<00:00,  1.32 iter/s, loss=0.464]\n",
      "Epoch 286: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.485]\n",
      "Epoch 287: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.313]\n",
      "Epoch 288: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.619]\n",
      "Epoch 289: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.291]\n",
      "Epoch 290: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.00051]\n",
      "Epoch 291: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000623]\n",
      "Epoch 292: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.226]\n",
      "Epoch 293: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.196]\n",
      "Epoch 294: 100%|██████████| 613/613 [10:15<00:00,  1.31 iter/s, loss=0.000756]\n",
      "Epoch 295: 100%|██████████| 613/613 [10:14<00:00,  1.32 iter/s, loss=0.000449]\n",
      "Epoch 296: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.0012]\n",
      "Epoch 297: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.251]\n",
      "Epoch 298: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.284]\n",
      "Epoch 299: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.000439]\n",
      "Epoch 300: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.213]\n",
      "Epoch 301: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.0944]\n",
      "Epoch 302: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.148]\n",
      "Epoch 303: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00117]\n",
      "Epoch 304: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.00124]\n",
      "Epoch 305: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.0018]\n",
      "Epoch 306: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00171]\n",
      "Epoch 307: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.00103]\n",
      "Epoch 308: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00104]\n",
      "Epoch 309: 100%|██████████| 613/613 [10:11<00:00,  1.33 iter/s, loss=0.000537]\n",
      "Epoch 310: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.418]\n",
      "Epoch 311: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.134]\n",
      "Epoch 312: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.765]\n",
      "Epoch 313: 100%|██████████| 613/613 [10:10<00:00,  1.31 iter/s, loss=0.000405]\n",
      "Epoch 314: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.0933]\n",
      "Epoch 315: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.417]\n",
      "Epoch 316: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.133]\n",
      "Epoch 317: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.149]\n",
      "Epoch 318: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.2]\n",
      "Epoch 319: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.371]\n",
      "Epoch 320: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.00147]\n",
      "Epoch 321: 100%|██████████| 613/613 [10:11<00:00,  1.33 iter/s, loss=0.558]\n",
      "Epoch 322: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.306]\n",
      "Epoch 323: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00234]\n",
      "Epoch 324: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.0013]\n",
      "Epoch 325: 100%|██████████| 613/613 [10:13<00:00,  1.33 iter/s, loss=0.000544]\n",
      "Epoch 326: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.000744]\n",
      "Epoch 327: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000446]\n",
      "Epoch 328: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.897]\n",
      "Epoch 329: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.000819]\n",
      "Epoch 330: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.38]\n",
      "Epoch 331: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.00147]\n",
      "Epoch 332: 100%|██████████| 613/613 [10:10<00:00,  1.31 iter/s, loss=0.166]\n",
      "Epoch 333: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000607]\n",
      "Epoch 334: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.000518]\n",
      "Epoch 335: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.932]\n",
      "Epoch 336: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.382]\n",
      "Epoch 337: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000386]\n",
      "Epoch 338: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.296]\n",
      "Epoch 339: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00039]\n",
      "Epoch 340: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.277]\n",
      "Epoch 341: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00114]\n",
      "Epoch 342: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.434]\n",
      "Epoch 343: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.502]\n",
      "Epoch 344: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000749]\n",
      "Epoch 345: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.655]\n",
      "Epoch 346: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.162]\n",
      "Epoch 347: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.229]\n",
      "Epoch 348: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000848]\n",
      "Epoch 349: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00132]\n",
      "Epoch 350: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=1.22]\n",
      "Epoch 351: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000367]\n",
      "Epoch 352: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00128]\n",
      "Epoch 353: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.264]\n",
      "Epoch 354: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.387]\n",
      "Epoch 355: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.462]\n",
      "Epoch 356: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00164]\n",
      "Epoch 357: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.477]\n",
      "Epoch 358: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00138]\n",
      "Epoch 359: 100%|██████████| 613/613 [10:12<00:00,  1.33 iter/s, loss=0.305]\n",
      "Epoch 360: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.577]\n",
      "Epoch 361: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.314]\n",
      "Epoch 362: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000383]\n",
      "Epoch 363: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.000739]\n",
      "Epoch 364: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00118]\n",
      "Epoch 365: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.000358]\n",
      "Epoch 366: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000353]\n",
      "Epoch 367: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.445]\n",
      "Epoch 368: 100%|██████████| 613/613 [10:09<00:00,  1.32 iter/s, loss=0.00129]\n",
      "Epoch 369: 100%|██████████| 613/613 [10:09<00:00,  1.32 iter/s, loss=0.196]\n",
      "Epoch 370: 100%|██████████| 613/613 [10:09<00:00,  1.33 iter/s, loss=0.000607]\n",
      "Epoch 371: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00344]\n",
      "Epoch 372: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.00106]\n",
      "Epoch 373: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00188]\n",
      "Epoch 374: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.000801]\n",
      "Epoch 375: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.394]\n",
      "Epoch 376: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.000303]\n",
      "Epoch 377: 100%|██████████| 613/613 [10:13<00:00,  1.32 iter/s, loss=0.00111]\n",
      "Epoch 378: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.421]\n",
      "Epoch 379: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.114]\n",
      "Epoch 380: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000572]\n",
      "Epoch 381: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.000356]\n",
      "Epoch 382: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.391]\n",
      "Epoch 383: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.375]\n",
      "Epoch 384: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.00105]\n",
      "Epoch 385: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.000506]\n",
      "Epoch 386: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.214]\n",
      "Epoch 387: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.00117]\n",
      "Epoch 388: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.247]\n",
      "Epoch 389: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.513]\n",
      "Epoch 390: 100%|██████████| 613/613 [10:12<00:00,  1.32 iter/s, loss=0.325]\n",
      "Epoch 391: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.000541]\n",
      "Epoch 392: 100%|██████████| 613/613 [10:11<00:00,  1.31 iter/s, loss=0.00038]\n",
      "Epoch 393: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.733]\n",
      "Epoch 394: 100%|██████████| 613/613 [10:10<00:00,  1.32 iter/s, loss=0.000698]\n",
      "Epoch 395: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.000389]\n",
      "Epoch 396: 100%|██████████| 613/613 [10:11<00:00,  1.32 iter/s, loss=0.388]\n",
      "Epoch 397: 100%|██████████| 613/613 [10:14<00:00,  1.31 iter/s, loss=0.184]\n",
      "Epoch 398: 100%|██████████| 613/613 [10:12<00:00,  1.31 iter/s, loss=0.000419]\n",
      "Epoch 399: 100%|██████████| 613/613 [10:13<00:00,  1.31 iter/s, loss=0.0889]\n"
     ]
    }
   ],
   "source": [
    "training = True\n",
    "\n",
    "counter = 0\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "for epoch in range(1, config.epoch):\n",
    "    with tqdm(total=len(loader), unit=' iter', unit_scale=False) as progress:\n",
    "        progress.set_description('Epoch %d' % epoch)\n",
    "        \n",
    "        with torch.set_grad_enabled(training):\n",
    "            for images, labels in loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                images = images.float()\n",
    "                input_image = images\n",
    "                images = images.to(device)\n",
    "\n",
    "                labels = labels.float()\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                output = model(images)\n",
    "                \n",
    "                if training:\n",
    "                    out_image = output.cpu().data\n",
    "                    label_image = labels.cpu().data\n",
    "                    \n",
    "                    weight = []\n",
    "                    for images in label_image.numpy().squeeze():\n",
    "                        weight_batch = []\n",
    "                        for image in images:\n",
    "                            value = np.ceil(image)\n",
    "                            if np.sum(image) > 0:\n",
    "                                value *= (np.sum(1-image) / np.sum(image))-1\n",
    "                            value += 1\n",
    "                            weight_batch.append(value)\n",
    "                        weight.append(weight_batch)\n",
    "                    \n",
    "                    weight = np.array(weight)\n",
    "                    \n",
    "                    criterion.weight = torch.tensor(weight).to(device)\n",
    "                    \n",
    "                    loss = criterion(output.squeeze(), labels.squeeze())\n",
    "                    loss = torch.mean(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    progress.set_postfix(loss=float(loss.item()))\n",
    "                    \n",
    "                    input_image = input_image.cpu().data\n",
    "                    input_image = input_image.view(-1, 256, 256)\n",
    "                    input_image = input_image.numpy()\n",
    "                    input_image = input_image.squeeze()\n",
    "                    input_image = [Image.fromarray(layer, 'F') for layer in input_image]\n",
    "                    input_image = [layer.resize((64, 64), Image.ANTIALIAS) for layer in input_image]\n",
    "                    input_image = [np.array(layer) for layer in input_image]\n",
    "                    input_image = np.asarray(input_image)\n",
    "                    input_image[input_image < 0] = 0\n",
    "                    input_image *= 255\n",
    "                    input_depth = len(input_image)\n",
    "                    input_image = input_image.repeat(3, axis=0)\n",
    "                    input_image = np.reshape(input_image, (input_depth, 3, 64, 64))\n",
    "                    \n",
    "                    out_image = out_image.view(-1, 64, 64)\n",
    "                    out_image = out_image.numpy()\n",
    "                    out_image = out_image.squeeze()\n",
    "                    out_depth = len(out_image)\n",
    "                    out_image = out_image.repeat(3, axis=0)\n",
    "                    out_image = np.reshape(out_image, (out_depth, 3, 64, 64))\n",
    "                    \n",
    "                    label_image = label_image.view(-1, 64, 64)\n",
    "                    label_image = label_image.numpy()\n",
    "                    label_image = label_image.squeeze()\n",
    "                    label_depth = len(label_image)\n",
    "                    label_image = label_image.repeat(3, axis=0)\n",
    "                    label_image = np.reshape(label_image, (label_depth, 3, 64,64))\n",
    "                    \n",
    "                    visual.images(\n",
    "                        tensor=input_image, nrow=9,\n",
    "                        win='input',\n",
    "                        opts=dict(title='input')\n",
    "                    )\n",
    "                    \n",
    "                    visual.images(\n",
    "                        tensor=out_image, nrow=9,\n",
    "                        win='output',\n",
    "                        opts=dict(title='output')\n",
    "                    )\n",
    "                    visual.images(\n",
    "                        tensor=label_image, nrow=9,\n",
    "                        win='label',\n",
    "                        opts=dict(title='label')\n",
    "                    )\n",
    "                    \n",
    "                    writer.add_scalar('data/loss', float(loss.item()), counter)\n",
    "\n",
    "                    progress.update(1)\n",
    "                    \n",
    "                    counter += 1\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception('There\\'s no Validation code available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_epoch = config.epoch\n",
    "#save_epoch = 0\n",
    "torch.save({\n",
    "    'epoch': save_epoch,\n",
    "    'state': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, 'save/3_'+str(save_epoch)+'_cb1.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = Model(64)\n",
    "pretrained_model = torch.load('save/3_200_cbSH.save')\n",
    "model.load_state_dict(pretrained_model['state'])\n",
    "\n",
    "model = model.eval()\n",
    "# cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "training = False\n",
    "\n",
    "visual = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    Reader.LiverData(\n",
    "        augment = False,\n",
    "        paths = ['Data/LiTS/Training/']\n",
    "    ),\n",
    "    1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 131/131 [05:32<00:00,  3.17s/ iter, loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOE:   0.6077362304257172\n",
      "DICE:  0.3922637715292943\n",
      "RVD:   1.2677130045692862\n",
      "0.3846000650073937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test a CT\n",
    "training = False\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "num_valid_data = 0\n",
    "meanVOE = 0\n",
    "meanDICE = 0\n",
    "meanRVD = 0\n",
    "DICE_list = []\n",
    "\n",
    "with tqdm(total=len(loader), unit=' iter', unit_scale=False) as progress:\n",
    "    progress.set_description('Testing')\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for image, label in loader:\n",
    "            \n",
    "            image = image.float()\n",
    "            image = image.squeeze()\n",
    "            \n",
    "            output = []\n",
    "            \n",
    "            for index in range(0, len(image), 3):\n",
    "                final = 0\n",
    "                if index+3 > len(image):\n",
    "                    final = index+3 - len(image)\n",
    "                    index = len(image) - 3\n",
    "                    \n",
    "                input_cell = image[index:index+3]\n",
    "                input_cell = input_cell.unsqueeze(0).unsqueeze(0)\n",
    "                input_cell = input_cell.to(device)\n",
    "                out_cell = model(input_cell)\n",
    "                out_cell = out_cell.squeeze()\n",
    "                if final is 0:\n",
    "                    feed = out_cell.cpu().data.numpy()\n",
    "                    for layer in feed:\n",
    "                        output.append(layer)\n",
    "                else:\n",
    "                    feed = out_cell.narrow(0, final, 3-final)\n",
    "                    feed = feed.cpu().data.numpy()\n",
    "                    for layer in feed:\n",
    "                        output.append(layer)\n",
    "            \n",
    "            label = label.float()\n",
    "            label = label.to(device)\n",
    "\n",
    "            if training:\n",
    "                raise Exception('There\\'s no Training code available.')\n",
    "            else:\n",
    "                input_image = image.cpu().data\n",
    "                input_image = input_image.view(-1, 256, 256)\n",
    "                input_image = input_image.numpy()\n",
    "                input_image = input_image.squeeze()\n",
    "                input_image = [Image.fromarray(layer, 'F') for layer in input_image]\n",
    "                input_image = [layer.resize((64, 64), Image.ANTIALIAS) for layer in input_image]\n",
    "                input_image = [np.array(layer) for layer in input_image]\n",
    "                input_image = np.asarray(input_image)\n",
    "                input_image[input_image < 0] = 0\n",
    "                input_image *= 255\n",
    "                input_depth = len(input_image)\n",
    "                input_image = input_image.repeat(3, axis=0)\n",
    "                input_image = np.reshape(input_image, (input_depth, 3, 64, 64))\n",
    "                \n",
    "                out_image = np.asarray(output)\n",
    "                out_image = out_image.reshape(-1, 64, 64)\n",
    "                out_image = out_image.squeeze()\n",
    "                out_depth = len(out_image)\n",
    "                out_image = out_image.repeat(3, axis=0)\n",
    "                out_image = np.reshape(out_image, (out_depth, 3, 64, 64))\n",
    "                \n",
    "                label_image = label.cpu().data\n",
    "                label_image = label_image.view(-1, 64, 64)\n",
    "                label_image = label_image.numpy()\n",
    "                label_image = label_image.squeeze()\n",
    "                label_depth = len(label_image)\n",
    "                label_image = label_image.repeat(3, axis=0)\n",
    "                label_image = np.reshape(label_image, (label_depth, 3, 64,64))\n",
    "                \n",
    "#                 for out_layer, label_layer in zip(out_image, label_image):\n",
    "#                     out_layer[0][:] = 0 #R\n",
    "#                     out_layer[1] = label_layer[1] #G\n",
    "#                     out_layer[2][:] = 0 #B\n",
    "\n",
    "                visual.images(\n",
    "                    tensor=input_image, nrow=9,\n",
    "                    win='input',\n",
    "                    opts=dict(title='input')\n",
    "                )\n",
    "                visual.images(\n",
    "                    tensor=out_image, nrow=9,\n",
    "                    win='output',\n",
    "                    opts=dict(title='output')\n",
    "                )\n",
    "                visual.images(\n",
    "                    tensor=label_image, nrow=9,\n",
    "                    win='label',\n",
    "                    opts=dict(title='label')\n",
    "                )\n",
    "\n",
    "                #threshold\n",
    "                out_image = np.ceil(out_image - 0.6)\n",
    "\n",
    "                a = np.sum(label_image)\n",
    "                b = np.sum(out_image)\n",
    "                aub = np.sum((label_image + out_image) / 2)\n",
    "                anb = np.sum(label_image * out_image)\n",
    "\n",
    "                if a > 1:\n",
    "                    #Volume Overlap Error\n",
    "                    VOE = 1 - (anb / aub)\n",
    "\n",
    "                    #DICE score\n",
    "                    DICE = (2 * anb) / (a + b)\n",
    "\n",
    "                    #Relative Volume Difference\n",
    "                    RVD = (b - a) / a\n",
    "\n",
    "                    meanVOE += VOE\n",
    "                    meanDICE += DICE\n",
    "                    meanRVD += RVD\n",
    "\n",
    "                    num_valid_data += 1\n",
    "                    \n",
    "                    DICE_list.append(DICE)\n",
    "                \n",
    "                progress.set_postfix(loss=float(DICE))\n",
    "\n",
    "                progress.update(1)\n",
    "\n",
    "meanVOE /= num_valid_data\n",
    "print('VOE:  ', meanVOE)\n",
    "meanDICE /= num_valid_data\n",
    "print('DICE: ', meanDICE)\n",
    "meanRVD /= num_valid_data\n",
    "print('RVD:  ', meanRVD)\n",
    "print(DICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for value in DICE_list:\n",
    "    print(value)\n",
    "    if value > 0.1:\n",
    "        num += 1\n",
    "print(sum(DICE_list)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "training = False\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "num_valid_data = 0\n",
    "meanVOE = 0\n",
    "meanDICE = 0\n",
    "meanRVD = 0\n",
    "\n",
    "with tqdm(total=len(loader), unit=' iter', unit_scale=False) as progress:\n",
    "    progress.set_description('Testing')\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        for image, label in loader:\n",
    "            \n",
    "            image = image.float()\n",
    "            image = image.to(device)\n",
    "            \n",
    "            label = label.float()\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "            \n",
    "            if training:\n",
    "                raise Exception('There\\'s no Training code available.')\n",
    "            else:\n",
    "                input_image = image.cpu().data\n",
    "                input_image = input_image.view(-1, 256, 256)\n",
    "                input_image = input_image.numpy()\n",
    "                input_image = input_image.squeeze()\n",
    "                input_image = [Image.fromarray(layer, 'F') for layer in input_image]\n",
    "                input_image = [layer.resize((64, 64), Image.ANTIALIAS) for layer in input_image]\n",
    "                input_image = [np.array(layer) for layer in input_image]\n",
    "                input_image = np.asarray(input_image)\n",
    "                input_image[input_image < 0] = 0\n",
    "                input_image *= 255\n",
    "                input_depth = len(input_image)\n",
    "                input_image = input_image.repeat(3, axis=0)\n",
    "                input_image = np.reshape(input_image, (input_depth, 3, 64, 64))\n",
    "                \n",
    "                out_image = output.cpu().data\n",
    "                out_image = out_image.view(-1, 64, 64)\n",
    "                out_image = out_image.numpy()\n",
    "                out_image = out_image.squeeze()\n",
    "                out_depth = len(out_image)\n",
    "                out_image = out_image.repeat(3, axis=0)\n",
    "                out_image = np.reshape(out_image, (out_depth, 3, 64, 64))\n",
    "                \n",
    "                label_image = label.cpu().data\n",
    "                label_image = label_image.view(-1, 64, 64)\n",
    "                label_image = label_image.numpy()\n",
    "                label_image = label_image.squeeze()\n",
    "                label_depth = len(label_image)\n",
    "                label_image = label_image.repeat(3, axis=0)\n",
    "                label_image = np.reshape(label_image, (label_depth, 3, 64,64))\n",
    "                \n",
    "#                 for out_layer, label_layer in zip(out_image, label_image):\n",
    "#                     out_layer[0][:] = 0 #R\n",
    "#                     out_layer[1] = label_layer[1] #G\n",
    "#                     out_layer[2][:] = 0 #B\n",
    "\n",
    "                visual.images(\n",
    "                    tensor=input_image, nrow=12,\n",
    "                    win='input',\n",
    "                    opts=dict(title='input')\n",
    "                )\n",
    "                visual.images(\n",
    "                    tensor=out_image, nrow=12,\n",
    "                    win='output',\n",
    "                    opts=dict(title='output')\n",
    "                )\n",
    "                visual.images(\n",
    "                    tensor=label_image, nrow=12,\n",
    "                    win='label',\n",
    "                    opts=dict(title='label')\n",
    "                )\n",
    "\n",
    "                #threshold\n",
    "                out_image = np.ceil(out_image - 0.5)\n",
    "\n",
    "                for label_layer, out_layer in zip(label_image, out_image):\n",
    "                    #BASIC values\n",
    "                    a = np.sum(label_layer)\n",
    "                    b = np.sum(out_layer)\n",
    "                    aub = np.sum((label_layer + out_layer) / 2)\n",
    "                    anb = np.sum(label_layer * out_layer)\n",
    "\n",
    "                    if a > 0:\n",
    "                        #Volume Overlap Error\n",
    "                        VOE = 1 - (anb / aub)\n",
    "\n",
    "                        #DICE score\n",
    "                        DICE = (2 * anb) / (a + b)\n",
    "\n",
    "                        #Relative Volume Difference\n",
    "                        RVD = (b - a) / a\n",
    "\n",
    "                        meanVOE += VOE\n",
    "                        meanDICE += DICE\n",
    "                        meanRVD += RVD\n",
    "\n",
    "                        num_valid_data += 1\n",
    "\n",
    "                progress.update(1)\n",
    "\n",
    "meanVOE /= num_valid_data\n",
    "print('VOE:  ', meanVOE)\n",
    "meanDICE /= num_valid_data\n",
    "print('DICE: ', meanDICE)\n",
    "meanRVD /= num_valid_data\n",
    "print('RVD:  ', meanRVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    Reader.Liver3Layer(\n",
    "        augment = False\n",
    "    ),\n",
    "    config.batch,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one part\n",
    "training = False\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "num_valid_data = 0\n",
    "meanVOE = 0\n",
    "meanDICE = 0\n",
    "meanRVD = 0\n",
    "\n",
    "with tqdm(total=len(loader), unit=' iter', unit_scale=False) as progress:\n",
    "    progress.set_description('Testing')\n",
    "\n",
    "    with torch.set_grad_enabled(training):\n",
    "        image, label = Reader.Liver3Layer(augment=False).__getitem__(5504)\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.float()\n",
    "        image = image.to(device)\n",
    "\n",
    "        label = label.unsqueeze(0)\n",
    "        label = label.float()\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "\n",
    "        if training:\n",
    "            raise Exception('There\\'s no Training code available.')\n",
    "        else:\n",
    "            input_image = image.cpu().data\n",
    "            input_image = input_image.view(-1, 256, 256)\n",
    "            input_image = input_image.numpy()\n",
    "            input_image = input_image.squeeze()\n",
    "            input_image = [Image.fromarray(layer, 'F') for layer in input_image]\n",
    "            input_image = [layer.resize((64, 64), Image.ANTIALIAS) for layer in input_image]\n",
    "            input_image = [np.array(layer) for layer in input_image]\n",
    "            input_image = np.asarray(input_image)\n",
    "            input_image[input_image < 0] = 0\n",
    "            input_image *= 255\n",
    "            input_depth = len(input_image)\n",
    "            input_image = input_image.repeat(3, axis=0)\n",
    "            input_image = np.reshape(input_image, (input_depth, 3, 64, 64))\n",
    "\n",
    "            out_image = output.cpu().data\n",
    "            out_image = out_image.view(-1, 64, 64)\n",
    "            out_image = out_image.numpy()\n",
    "            out_image = out_image.squeeze()\n",
    "            out_depth = len(out_image)\n",
    "            out_image = out_image.repeat(3, axis=0)\n",
    "            out_image = np.reshape(out_image, (out_depth, 3, 64, 64))\n",
    "\n",
    "            label_image = label.cpu().data\n",
    "            label_image = label_image.view(-1, 64, 64)\n",
    "            label_image = label_image.numpy()\n",
    "            label_image = label_image.squeeze()\n",
    "            label_depth = len(label_image)\n",
    "            label_image = label_image.repeat(3, axis=0)\n",
    "            label_image = np.reshape(label_image, (label_depth, 3, 64,64))\n",
    "\n",
    "#                 for out_layer, label_layer in zip(out_image, label_image):\n",
    "#                     out_layer[0][:] = 0 #R\n",
    "#                     out_layer[1] = label_layer[1] #G\n",
    "#                     out_layer[2][:] = 0 #B\n",
    "\n",
    "            visual.images(\n",
    "                tensor=input_image, nrow=12,\n",
    "                win='input',\n",
    "                opts=dict(title='input')\n",
    "            )\n",
    "            visual.images(\n",
    "                tensor=out_image, nrow=12,\n",
    "                win='output',\n",
    "                opts=dict(title='output')\n",
    "            )\n",
    "            visual.images(\n",
    "                tensor=label_image, nrow=12,\n",
    "                win='label',\n",
    "                opts=dict(title='label')\n",
    "            )\n",
    "\n",
    "            #threshold\n",
    "            out_image = np.ceil(out_image - 0.5)\n",
    "\n",
    "            for label_layer, out_layer in zip(label_image, out_image):\n",
    "                    #BASIC values\n",
    "                    a = np.sum(label_layer)\n",
    "                    b = np.sum(out_layer)\n",
    "                    aub = np.sum((label_layer + out_layer) / 2)\n",
    "                    anb = np.sum(label_layer * out_layer)\n",
    "\n",
    "                    if a > 0:\n",
    "                        #Volume Overlap Error\n",
    "                        VOE = 1 - (anb / aub)\n",
    "\n",
    "                        #DICE score\n",
    "                        DICE = (2 * anb) / (a + b)\n",
    "\n",
    "                        #Relative Volume Difference\n",
    "                        RVD = (b - a) / a\n",
    "\n",
    "                        meanVOE += VOE\n",
    "                        meanDICE += DICE\n",
    "                        meanRVD += RVD\n",
    "\n",
    "                        num_valid_data += 1\n",
    "\n",
    "            progress.update(1)\n",
    "\n",
    "meanVOE /= num_valid_data\n",
    "print('VOE:  ', meanVOE)\n",
    "meanDICE /= num_valid_data\n",
    "print('DICE: ', meanDICE)\n",
    "meanRVD /= num_valid_data\n",
    "print('RVD:  ', meanRVD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
